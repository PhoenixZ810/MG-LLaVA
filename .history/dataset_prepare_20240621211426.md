# Dataset Preparation

## Pretraining

During the image-based training stage, our dataset comprises 558K image-caption pairs from LAION-CCSBU and 708k image-caption pairs from ALLaVA-4V-Caption dataset, culminating in a total of 1.26M image-caption pairs for pretraining.

## Fine-tuning

The datasets employed for instruction-tuning encompass 665K mixture dataset from LLaVA-Instruct, 692k instructions from ALLaVA-4V-Instruction dataset, and an additional 25k instructions derived from a combination of ShareGPT4V , DocVQA, DVQA and AI2D, with a total number of more than 1.3M image-text conversations.

## Download
Download llava-1.5 pretrain images from [here](https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K), and downlaod ALLaVA-4V-LAION and ALLaVA-4V-Vison-FLAN images from [here](https://huggingface.co/datasets/FreedomIntelligence/ALLaVA-4V). Then download [COCO], [GQA], [OCR-VQA], [TextVQA], [VG] following LLaVA-1.5. The Web-Celebrity, [Web-Landmark, WikiArt],  Share-TextVQA can be found in [ShareGPT-4V]()[AI2D], [DocVQA], [DVQA],

The complete structure is as follows:
```text
MG-LLAVA
├── data
│   ├── LLaVA-Pretrain
│   │   ├── images
│   ├── ai2d
|   |   ├── images
│   ├── allava_laion
|   |   ├── images
│   ├── allava_vflan
|   |   ├── images
│   ├── coco
|   |   ├── train2017
│   ├── docvqa
|   |   ├── images
│   ├── dvqa
|   |   ├── images
│   ├── gqa
|   |   ├── images
│   ├── ocr_vqa
|   |   ├── images
│   ├── share_textvqa
|   |   ├── images
│   ├── textvqa
|   |   ├── train_images
│   ├── vg
|   |   ├── VG_100K
|   |   ├── VG_100K_2
│   ├── web-celebrity
|   |   ├── images
│   ├── web-landmark
|   |   ├── images
│   ├── wikiart
|   |   ├── images
```