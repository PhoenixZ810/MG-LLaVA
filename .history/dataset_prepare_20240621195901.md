# D

During the image-based training stage, our dataset comprises 558K image-caption pairs
from LAION-CCSBU [60 ] and 708k image-caption pairs from ALLaVA-4V-Caption dataset [61 ],
culminating in a total of 1.26M image-caption pairs for pretraining. The datasets employed for
instruction-tuning encompass 665K mixture dataset from LLaVA-Instruct [ 30 ], 692k instructions from
ALLaVA-4V-Instruction dataset [61 ], and an additional 25k instructions derived from a combination
of ShareGPT4V [62 ], DocVQA [ 63 ], DVQA [ 64 ] and AI2D [ 65 ], with a total number of more
than 1.3M image-text conversations.