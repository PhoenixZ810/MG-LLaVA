# Dataset Preparation

## Pretraining

During the image-based training stage, our dataset comprises 558K image-caption pairs from LAION-CCSBU and 708k image-caption pairs from ALLaVA-4V-Caption dataset, culminating in a total of 1.26M image-caption pairs for pretraining.

## Fine-tuning

The datasets employed for instruction-tuning encompass 665K mixture dataset from LLaVA-Instruct, 692k instructions from ALLaVA-4V-Instruction dataset, and an additional 25k instructions derived from a combination of ShareGPT4V , DocVQA, DVQA and AI2D, with a total number of more than 1.3M image-text conversations.

## Download
Download llava pretrain images from [here](https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K), download

The complete structure is as follows:
```text
MG-LLAVA
├── data
│   ├── LLaVA-Pretrain
│   ├── ai2d
│   ├── allava_laion
│   ├── allava_vflan
│   ├── coco
│   ├── docvqa
│   ├── dvqa
│   ├── gqa
│   ├── share_textvqa
│   ├── textvqa
│   ├── vg
```